#!/usr/bin/env python3

# mtu - media tracker utility

import csv
import json
import logging as log
import os
import socket

from datetime import timezone
from enum import Enum
from collections import OrderedDict, namedtuple
from hashlib import sha256
from urllib.request import urlopen, Request, URLError
from urllib.parse import urlparse

from bs4 import BeautifulSoup
from iso8601 import parse_date, ParseError

class Config:
    data_dir        = "data"
    cache_dir       = "cache"
    user_agent      = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0"
    import_file     = "urls.list"
    main_csv        = "media.csv"
    log_file        = "mtu.log"
    conn_timeout    = 10
    conn_blacklist  = [
        "nasdaq.com",   # doesn't serve response to Python or wget
    ]

class Fields(Enum):
    """Standard fields we'll try to populate from all the various ways to store metadata used on the web.

    Members of this enum are used as dict keys.
    """
    url             = "url"
    title           = "title"
    author          = "author"
    published_utc   = "published_utc"
    updated_utc     = "updated_utc"

class Src(Enum):
    """Source elements we'll take metadata from.

    Src lists elements commonly used to store metadata. Members with cust_ prefix refer to elements known by custom per-site code to extract metadata. Members of this enum are used as dict keys. String values are hints to source elements.
    """
    # canonical link from head>link[rel=canonical][href]
    head_canonical      = "canonical"
    # url from meta[property=og:url][content], Open Graph
    og_url              = "og:url"
    # custom URL location
    cust_url            = "cust_url"
    # title from meta[property=og:title][content], Open Graph
    og_title            = "og:title"
    # title from head>title
    head_title          = "title"
    # custom title location
    cust_title          = "cust_title"
    # author name from meta[name=author][content], https://www.w3schools.com/tags/tag_meta.asp
    head_author         = "meta[name=author]"
    # custom author location
    cust_author         = "cust_author"
    # published time from meta[property=article:published_time][content], Open Graph
    og_published_time   = "article:published_time"
    # published time from meta[itemprop=datePublished][content], schema.org
    so_date_published   = "schema:datePublished"
    # custom published_utc location
    cust_published_time = "cust_published_time"
    # updated time from meta[property=article:modified_time][content], Open Graph
    og_modified_time    = "article:modified_time"
    # modified time from meta[itemprop=dateModified][content], schema.org
    so_date_modified    = "schema:dateModified"
    # custom updated_utc location
    cust_updated_time   = "cust_updated_time"

# Order in which source meta values are considered.
# Custom sources have top priority because custom per-site code knows best where the best meta values are stored.
SRC_PRIORITIES = {
    Fields.url           : [ Src.cust_url, Src.head_canonical ],
    Fields.title         : [ Src.cust_title, Src.og_title, Src.head_title ],
    Fields.author        : [ Src.cust_author, Src.head_author ],
    Fields.published_utc : [ Src.cust_published_time, Src.og_published_time, Src.so_date_published ],
    Fields.updated_utc   : [ Src.cust_updated_time, Src.og_modified_time, Src.so_date_modified ],
}

def value(bstag, attr=None):
    """Return the value of a bs4 tag or its attribute. None is returned if the tag or attribute is None."""
    if bstag:
        if attr:
            return bstag.get(attr)
        else:
            return bstag.string
    return None

def canonical(bs):
    return value(bs.find("link", attrs={"rel": "canonical"}), "href")

def common_meta_values(bs):
    """Extract values from common metadata elements listed in Src."""
    return {
        Src.head_canonical    : canonical(bs),
        Src.og_url            : value(bs.find("meta", attrs={"property": "og:url"}), "content"),
        Src.og_title          : value(bs.find("meta", attrs={"property": "og:title"}), "content"),
        Src.head_title        : value(bs.head.title),
        Src.head_author       : value(bs.find("meta", attrs={"name": "author"}), "content"),
        Src.og_published_time : value(bs.find("meta", attrs={"property": "article:published_time"}), "content"),
        Src.so_date_published : value(bs.find("meta", attrs={"itemprop": "datePublished"}), "content"),
        Src.og_modified_time  : value(bs.find("meta", attrs={"property": "article:modified_time"}), "content"),
        Src.so_date_modified  : value(bs.find("meta", attrs={"itemprop": "dateModified"}), "content"),
    }

CUSTOM_META_EXTRACTORS = {
    "bitcoinist.com": lambda bs: {
        # flaky, replace with LD
        Src.cust_author         : value(bs.find("div", attrs={"class": "meta"}).find("p").find("a")),
    },
    "captainaltcoin.com": lambda bs: {
        # not bad but LD is better
        Src.cust_author         : value(bs.find("a", attrs={"rel": "author"})),
    },
    "coindesk.com": lambda bs: {
        # flaky, replace with LD
        Src.cust_author         : value(bs.find("section", attrs={"class": "author"})
                                          .find("h5", attrs={"class": "heading"})),
    },
    "forbes.com": lambda bs: {
        # dates in LD are more precise than schema.org but not ISO 8601
    },
    "news.bitcoin.com": lambda bs: {
        # flaky, LD is better
        Src.cust_author         : value(bs.find("p", attrs={"class": "article__info__author"}).find("strong")),
    },
    "thehackernews.com": lambda bs: {
        # schema.org metadata embedded in microdata
        Src.cust_author         : value(bs.find("div", attrs={"itemtype": "http://schema.org/Person"})
                                          .find("meta", attrs={"itemprop": "name"}), "content"),
    },
    "themerkle.com": lambda bs: {
        Src.cust_author         : value(bs.find("a", attrs={"rel": "author"})),
    }
}

def get_domain(url):
    hn = urlparse(url).hostname
    nowww = hn[len("www."):] if hn.startswith("www.") else hn
    return nowww

def custom_meta_values(bs, domain):
    custom_ex = CUSTOM_META_EXTRACTORS.get(domain)
    if custom_ex:
        return custom_ex(bs)
    else:
        return None

def make_multimeta(src_values):
    """Multimeta groups source values by target Fields, sorts values in each group by their priority, and removes None values."""
    multimeta = {}
    for field in Fields:
        values = OrderedDict()
        for src_el in SRC_PRIORITIES[field]:
            val = src_values.get(src_el)
            if val: # only include non-None values
                values[src_el] = val
        multimeta[field] = values
    return multimeta

def normalize_iso_date(date_str):
    """Normalize various possible ISO 8601 date strings to '2020-01-01 10:34:47'"""
    try:
        # first convert to UTC, then drop tzinfo to remove tz offset from the formatted string
        return parse_date(date_str).astimezone(timezone.utc).replace(tzinfo=None).isoformat(' ')
    except ParseError:
        return date_str

def first_value(dict_, default=None):
    try:
        return next(iter(dict_.values()))
    except StopIteration:
        return default

def best_normalized_values(multimeta):
    """Pick first value from each OrderedDict storing meta value candidates, normalize date values."""
    best = {}
    for field, values in multimeta.items():
        best_val = first_value(values)
        if field in [Fields.published_utc, Fields.updated_utc]:
            best_val = normalize_iso_date(best_val)
        best[field] = best_val
    return best

def print_source_values(src_meta): # dict(Src -> str)
    """Print non-None source meta values in standard order."""
    for src_el in Src:
        val = src_meta.get(src_el)
        if val:
            print("  {:22}: {}".format(src_el.value, val))

def print_single_values(meta): # dict(Field -> str)
    """Print non-None standard meta values in standard order."""
    for field in Fields:
        val = meta.get(field)
        if val:
            print("  {:22}: {}".format(field.name, val))

def print_multimeta(multimeta): # dict(Field -> OrderedDict(Src -> str))
    for field in Fields:
        src_values = multimeta.get(field)
        if src_values:
            print_source_values(src_values)

class CommandError(Exception):
    """Raise this when command is called by the user incorrectly."""
    pass

def read_lines(path):
    with open(path) as f:
        return [line.rstrip('\n') for line in f]

def make_bs(filename):
    with open(filename) as html_file:
        return BeautifulSoup(html_file, "html.parser")

def cmd_getmeta(args):
    filenames = []
    if args.file:
        filenames.append(args.file)
    if args.list:
        listed_files = read_lines(args.list)
        filenames.extend(listed_files)

    for fname in filenames:
        print("--", fname)
        bs = make_bs(fname)

        src_values = common_meta_values(bs)
        print("COMMON meta values:")
        print_source_values(src_values)

        url = src_values[Src.head_canonical] # URL might be externally provided later
        if url:
            domain = get_domain(url)
            custom_values = custom_meta_values(bs, domain)
            if custom_values:
                print("CUSTOM meta values for {}: ".format(domain))
                print_source_values(custom_values)
                src_values.update(custom_values)

        # multimeta construction is redundant now but might be useful in the future. we'll see
        multimeta = make_multimeta(src_values)

        best = best_normalized_values(multimeta)
        print("BEST normalized meta values:")
        print_single_values(best)

def cmd_inspect(args):
    bs = make_bs(args.file)
    print("-- tags with itemprop attribute from schema.org:")
    for tag in bs.find_all(True, attrs={"itemprop": True}):
        print(tag)
    print("-- ld+json")
    for tag in bs.find_all("script", attrs={"type": "application/ld+json"}):
        print(tag.name, tag.attrs)
        ld = json.loads(tag.string)
        print(json.dumps(ld, indent=4, sort_keys=True))

def init():
    if not os.path.isdir(Config.cache_dir):
        os.mkdir(Config.cache_dir)

def cache_id(url):
    # mind no newline is added so the hash may differ from a typical sha256sum use
    return sha256(url.encode()).hexdigest()

def cache_exists(cid):
    return os.path.isfile(os.path.join(Config.cache_dir, cid))

def cache_save(cid, bytes_):
    try:
        with open(os.path.join(Config.cache_dir, cid), "xb") as f:
            f.write(bytes_)
        log.info("cached with id: %s", cid)
    except FileExistsError as e:
        log.warning("already cached with id: %s", cid)

def cache_load_bs(cid):
    """Build a BeatutifulSoup instance from the cached file."""
    return make_bs(os.path.join(Config.cache_dir, cid))

CacheHit = namedtuple("CacheHit", ["url", "cache_id"])
FetchResult = namedtuple("FetchResult", ["url", "cache_id", "bs"])

def fetch_url(url):
    log.info("checking: %s", url)
    existing_cid = cache_id(url)
    if cache_exists(existing_cid):
        log.info("found in cache with id: %s", existing_cid)
        return CacheHit(url = url, cache_id = existing_cid)
    domain = get_domain(url)
    if domain in Config.conn_blacklist:
        log.info("not connecting to blacklisted domain: %s", domain)
        return None
    req = Request(url, headers={"User-Agent": Config.user_agent})
    log.info("fetching: %s", url)
    try:
        with urlopen(req, timeout=Config.conn_timeout) as resp:
            bytes_ = resp.read()
    except (socket.timeout, URLError) as e:
        log.error(e)
        return e
    final_url = resp.geturl()
    if url != final_url:
        log.warning("redirected to: %s", final_url)
    bs = BeautifulSoup(bytes_, "html.parser") # todo: don't construct bs in fetch at all
    cid = cache_id(final_url)
    cache_save(cid, bytes_)
    return FetchResult(url = final_url, cache_id = cid, bs = bs)

def load_url(url):
    """Like fetch_url, but also load the cached file if there was a cache hit."""
    res = fetch_url(url)
    if isinstance(res, CacheHit):
        bs = cache_load_bs(res.cache_id)
        return FetchResult(url = res.url, cache_id = res.cache_id, bs = bs)
    return res

def csv_iter(csvfilename):
    """Generator yielding rows of the csv file."""
    with open(csvfilename, newline='') as csvfile:
        yield from csv.reader(csvfile)

class CsvRow:
    """Indexes of fields in the csv row/record."""
    url         = 1
    dead_link   = 9

class CsvConst:
    false   = "no"
    true    = "yes"

def fetch_from_csv(csvfilename):
    log.info("fetching URLs in %s", csvfilename)
    for row in csv_iter(csvfilename):
        fetch_url(row[CsvRow.url])

def update_csv(csvfilename):
    log.info("updating metadata in %s", csvfilename)
    new_rows = []
    csvit = csv_iter(csvfilename)
    try:
        header = next(csvit) # save header for writing it back later
    except StopIteration:
        log.error("empty csv file: %s", csvfilename)
        return

    for row in csvit:
        url = row[CsvRow.url]
        if row[CsvRow.dead_link] == CsvConst.true:
            log.info("skipping dead link: %s", url)
        else:
            fr = load_url(url)
            # inspect fetch result and see which cells of the row need updating
            # mutate rows in-place
            if isinstance(fr, FetchResult):
                if url != fr.url:
                    row[CsvRow.url] = fr.url
                # inspect canonical
                # we can't automatically determine whether to use canonical link because
                # it is not properly populated on some sites
                canon = canonical(fr.bs)
                if canon:
                    if canon != fr.url:
                        log.warning("canonical link is different from the final URL: %s", canon)
                else:
                    log.warning("canonical link is missing")
            elif isinstance(fr, Exception):
                row[CsvRow.dead_link] = CsvConst.true
                log.info("could not fetch, marked as dead link: %s", url)
        new_rows.append(row)

    # overwrite existing file
    with open(csvfilename, "w", newline="") as outfile:
        writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL, lineterminator="\n")
        writer.writerow(header)
        writer.writerows(new_rows)
    log.info("file updated: %s", csvfilename)

def cmd_fetch(args):
    init()
    if args.csv:
        fetch_from_csv(Config.main_csv)
    else:
        fetch_url(args.url)

def cmd_update(args):
    init()
    update_csv(Config.main_csv)

def make_arg_parser():
    import argparse

    parser = argparse.ArgumentParser(description="media tracker utility")
    subparsers = parser.add_subparsers(dest="command", title="commands")

    parser_meta = subparsers.add_parser("meta", help="print metadata extracted from HTML file")
    parser_meta.add_argument("file", nargs='?', help="file to process")
    parser_meta.add_argument("--list", nargs='?', const=Config.import_file,
                                help="list of files to process")
    parser_meta.set_defaults(func=cmd_getmeta)

    parser_inspect = subparsers.add_parser("inspect", help="inspect HTML file for possibly interesting metadata")
    parser_inspect.add_argument("file", nargs='?', help="file to process")
    parser_inspect.set_defaults(func=cmd_inspect)

    parser_fetch = subparsers.add_parser("fetch", help="fetch URL(s) and cache the HTML")
    fetch_group = parser_fetch.add_mutually_exclusive_group(required=True)
    fetch_group.add_argument("url", nargs='?', help="URL to fetch")
    fetch_group.add_argument("--csv", action="store_true", help="read URLs from " + Config.main_csv)
    parser_fetch.set_defaults(func=cmd_fetch)

    parser_update = subparsers.add_parser("update", help="update metadata in a " + Config.main_csv + " file, fetching URLs as necessary")
    parser_update.set_defaults(func=cmd_update)

    return parser

def main():
    log.basicConfig(level=log.INFO,
                    datefmt="%Y-%m-%d %H:%M:%S",
                    format="%(asctime)s.%(msecs)03d [%(levelname)-4.4s] %(message)s",
                    handlers=[
                        log.FileHandler(Config.log_file),
                        log.StreamHandler(),
                    ])

    parser = make_arg_parser()
    args = parser.parse_args()

    if args.command:
        try:
            args.func(args)
        except CommandError as e:
            print("error:", e)
        except KeyboardInterrupt as e:
            print("aborting via Ctrl-C")
        except BrokenPipeError:
            # silence error when e.g. piping into `less` and quitting before reading all
            pass
    else:
        parser.print_usage()

if __name__ == "__main__":
    main()
